"""
Performance Tracker - PRODUCTION VERSION
Tracks REAL model performance from production data
"""

import logging
import json
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional
import pandas as pd
import numpy as np
from sklearn.metrics import f1_score

logger = logging.getLogger(__name__)

# Performance thresholds (UPDATED for realistic models)
QUALITY_F1_THRESHOLD = 0.70  # Changed from 0.95
FAILURE_F1_THRESHOLD = 0.65  # Changed from 0.90
CONSECUTIVE_DROPS = 3

def convert_to_python_types(obj):
    """Convert NumPy types to Python native types"""
    if isinstance(obj, np.bool_):
        return bool(obj)
    elif isinstance(obj, (np.integer, np.int64, np.int32)):
        return int(obj)
    elif isinstance(obj, (np.floating, np.float64, np.float32)):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {key: convert_to_python_types(val) for key, val in obj.items()}
    elif isinstance(obj, list):
        return [convert_to_python_types(item) for item in obj]
    return obj

class PerformanceTracker:
    """Track REAL model performance from production data"""
    
    def __init__(self, 
                 metrics_file: str = "reports/performance_history.json",
                 production_data_path: str = "data/production/labeled_predictions.csv",
                 validation_f1_quality: float = 0.771,
                 validation_f1_failure: float = 0.709):
        self.metrics_file = Path(metrics_file)
        self.metrics_file.parent.mkdir(parents=True, exist_ok=True)
        self.production_data_path = Path(production_data_path)
        
        # Fallback validation scores (from your trained models)
        self.validation_f1_quality = validation_f1_quality
        self.validation_f1_failure = validation_f1_failure
        
        self.history = self._load_history()
    
    def _load_history(self) -> List[Dict]:
        if self.metrics_file.exists():
            try:
                with open(self.metrics_file, 'r') as f:
                    return json.load(f)
            except Exception as e:
                logger.error(f"Failed to load history: {e}")
                return []
        return []
    
    def _save_history(self):
        try:
            history_cleaned = convert_to_python_types(self.history)
            with open(self.metrics_file, 'w') as f:
                json.dump(history_cleaned, f, indent=2)
            logger.info(f"Saved performance history to {self.metrics_file}")
        except Exception as e:
            logger.error(f"Failed to save history: {e}")
    
    def get_production_performance(self, hours=24):
        """
        Get REAL F1 scores from production data
        
        Args:
            hours: Look back window for production data
            
        Returns:
            (quality_f1, failure_f1) tuple
        """
        try:
            # Try to load production predictions with ground truth
            if not self.production_data_path.exists():
                logger.warning(f"Production data not found: {self.production_data_path}")
                logger.info("Using validation F1 scores as baseline")
                return self.validation_f1_quality, self.validation_f1_failure
            
            df = pd.read_csv(self.production_data_path)
            
            # Filter recent data
            if 'timestamp' in df.columns:
                df['timestamp'] = pd.to_datetime(df['timestamp'])
                cutoff = datetime.now() - timedelta(hours=hours)
                df = df[df['timestamp'] > cutoff]
            
            # Check if we have ground truth labels
            if len(df) < 100:
                logger.warning(f"Only {len(df)} labeled samples, need >100. Using validation baseline.")
                return self.validation_f1_quality, self.validation_f1_failure
            
            # Calculate REAL F1 scores
            quality_f1 = f1_score(df['true_quality_label'], df['pred_quality'])
            failure_f1 = f1_score(df['true_failure_label'], df['pred_failure'])
            
            logger.info(f"‚úÖ Calculated from {len(df)} production samples")
            
            return float(quality_f1), float(failure_f1)
            
        except Exception as e:
            logger.warning(f"Failed to get production performance: {e}")
            logger.info("Falling back to validation F1 scores")
            return self.validation_f1_quality, self.validation_f1_failure
    
    def record_performance(self, 
                          quality_f1: Optional[float] = None,
                          failure_f1: Optional[float] = None,
                          timestamp: Optional[str] = None) -> Dict:
        """
        Record model performance
        If F1 scores not provided, fetches from production data
        """
        if timestamp is None:
            timestamp = datetime.now().isoformat()
        
        # Get actual performance if not provided
        if quality_f1 is None or failure_f1 is None:
            logger.info("Fetching actual production performance...")
            quality_f1, failure_f1 = self.get_production_performance()
        
        record = {
            'timestamp': timestamp,
            'quality_f1': float(quality_f1),
            'failure_f1': float(failure_f1),
            'quality_degraded': bool(quality_f1 < QUALITY_F1_THRESHOLD),
            'failure_degraded': bool(failure_f1 < FAILURE_F1_THRESHOLD),
            'source': 'production' if self.production_data_path.exists() else 'validation'
        }
        
        self.history.append(record)
        self._save_history()
        
        logger.info(f"Recorded performance: Quality F1={quality_f1:.4f}, Failure F1={failure_f1:.4f}")
        
        return record
    
    def check_degradation(self) -> Dict:
        """Check if model performance has degraded"""
        if len(self.history) < CONSECUTIVE_DROPS:
            return {
                'quality_degraded': False,
                'failure_degraded': False,
                'consecutive_drops': 0,
                'should_retrain': False
            }
        
        recent = self.history[-CONSECUTIVE_DROPS:]
        
        quality_drops = sum(1 for r in recent if r['quality_degraded'])
        failure_drops = sum(1 for r in recent if r['failure_degraded'])
        
        should_retrain = (quality_drops >= CONSECUTIVE_DROPS or 
                         failure_drops >= CONSECUTIVE_DROPS)
        
        result = {
            'quality_degraded': bool(quality_drops >= CONSECUTIVE_DROPS),
            'failure_degraded': bool(failure_drops >= CONSECUTIVE_DROPS),
            'consecutive_drops': int(max(quality_drops, failure_drops)),
            'should_retrain': bool(should_retrain)
        }
        
        if should_retrain:
            logger.warning(f"‚ö†Ô∏è Performance degradation detected! {result}")
        
        return result
    
    def get_current_performance(self) -> Optional[Dict]:
        if not self.history:
            return None
        return convert_to_python_types(self.history[-1])
    
    def get_performance_summary(self) -> Dict:
        if not self.history:
            return {}
        
        df = pd.DataFrame(self.history)
        
        summary = {
            'total_records': int(len(self.history)),
            'quality_f1_mean': float(df['quality_f1'].mean()),
            'quality_f1_std': float(df['quality_f1'].std()),
            'quality_f1_min': float(df['quality_f1'].min()),
            'quality_f1_current': float(df['quality_f1'].iloc[-1]),
            'failure_f1_mean': float(df['failure_f1'].mean()),
            'failure_f1_std': float(df['failure_f1'].std()),
            'failure_f1_min': float(df['failure_f1'].min()),
            'failure_f1_current': float(df['failure_f1'].iloc[-1]),
            'degradation_events': int(df['quality_degraded'].sum() + df['failure_degraded'].sum())
        }
        
        return convert_to_python_types(summary)


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    
    tracker = PerformanceTracker()
    
    # Get actual performance (will use validation baseline if no production data)
    logger.info("Checking production performance...")
    quality_f1, failure_f1 = tracker.get_production_performance()
    
    record = tracker.record_performance(quality_f1, failure_f1)
    print(f"\nüìä Performance Recorded:")
    print(json.dumps(record, indent=2))
    
    degradation = tracker.check_degradation()
    print(f"\nüîç Degradation Check:")
    print(json.dumps(degradation, indent=2))
    
    if degradation['should_retrain']:
        print("\nüö® ACTION REQUIRED: Model retraining triggered!")
        print(f"Quality F1: {quality_f1:.4f} < {QUALITY_F1_THRESHOLD} threshold")
        print(f"Failure F1: {failure_f1:.4f} < {FAILURE_F1_THRESHOLD} threshold")
    else:
        print("\n‚úÖ Performance within acceptable range")
        print(f"Quality F1: {quality_f1:.4f} (threshold: {QUALITY_F1_THRESHOLD})")
        print(f"Failure F1: {failure_f1:.4f} (threshold: {FAILURE_F1_THRESHOLD})")
    
    summary = tracker.get_performance_summary()
    print(f"\nüìà Performance Summary:")
    print(json.dumps(summary, indent=2))